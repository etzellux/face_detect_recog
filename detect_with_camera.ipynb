{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ed717f-96b6-4a2f-8bc1-8361e0217e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy import asarray\n",
    "from PIL import Image\n",
    "from keras_vggface.utils import preprocess_input\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from scipy.spatial.distance import cosine\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f39e6b-ea03-40f0-9e3d-4e80af831af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution() #optimization purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a04c2a0-1ddd-4069-91e0-7ac7bb5529b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def highlight_faces(image, faces):\n",
    "  \n",
    "    for face in faces:\n",
    "        x, y, width, height = face['box']\n",
    "        cv2.rectangle(image,(x,y),(x + width, y + height),color=(0,0,255),thickness=4)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "478babd0-5bcf-4454-be15-0478f0e748ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_res(cap, width, height):\n",
    "    cap.set(3, width)\n",
    "    cap.set(4, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77074b7e-c664-4eaa-b389-48a5be0b6674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_from_image(image, face_infos, required_size=(224, 224)):\n",
    "  # load image and detect faces\n",
    "  \n",
    "    face_images = []\n",
    "\n",
    "    for face in face_infos:\n",
    "        # extract the bounding box from the requested face\n",
    "        x1, y1, width, height = face['box']\n",
    "        x2, y2 = x1 + width, y1 + height\n",
    "\n",
    "        # extract the face\n",
    "        face_boundary = image[y1:y2, x1:x2]\n",
    "\n",
    "        # resize pixels to the model size\n",
    "        face_image = Image.fromarray(face_boundary)\n",
    "        face_image = face_image.resize(required_size)\n",
    "        face_array = asarray(face_image)\n",
    "        face_images.append(face_array)\n",
    "\n",
    "    return face_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98e72e4b-ed00-4521-bf37-0fdada574be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tfsla\\anaconda3\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:532: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = VGGFace(model='resnet50',\n",
    "      include_top=False,\n",
    "      input_shape=(224, 224, 3),\n",
    "      pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb5431c3-bfd0-4b36-84a0-54d9c2e1d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face_mapping(faces, target_face):\n",
    "    faces = np.append(faces, target_face, axis=0)\n",
    "    samples = asarray(faces, 'float32')\n",
    "\n",
    "  # prepare the data for the model\n",
    "    samples = preprocess_input(samples, version=2)\n",
    "\n",
    "  # perform prediction\n",
    "    return model.predict(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3c0fa2f-5075-44a8-9228-f9d3b4c9a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aef346eb-0c98-4420-94d8-b8db7acc17a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n",
      "no face matched\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "change_res(cap, 200, 130)\n",
    "detector = MTCNN()\n",
    "\n",
    "target = cv2.imread(\"images/blade_runner2049_2.jpg\") # read students face\n",
    "target = cv2.cvtColor(target, cv2.COLOR_BGR2RGB)\n",
    "target_face_info = detector.detect_faces(target) # get face boundaries\n",
    "\n",
    "target_face = np.array(extract_face_from_image(target, target_face_info)) # extract face and append to array\n",
    "\n",
    "face_matched = False\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read() # read from camera\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    #detect faces\n",
    "    face_infos = detector.detect_faces(frame_rgb) \n",
    "    face_infos = np.array(face_infos)\n",
    "    \n",
    "    if len(face_infos) > 0 :\n",
    "        \n",
    "        face_images = extract_face_from_image(frame_rgb, face_infos)\n",
    "        # TODO: flush faces except target in every loop                   \n",
    "        #faces = np.append(face_infos, target_face, axis=0)\n",
    "        \n",
    "        face_vectors = get_face_mapping(face_images, target_face)\n",
    "         \n",
    "        face_count = face_vectors.shape[0]\n",
    "                                \n",
    "        for i in range(0, face_count):\n",
    "            for j in range(i, face_count):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if cosine(face_vectors[i], face_vectors[j]) <= threshold:\n",
    "                    print(\"Face \" + str(i) + \" and Face \" + str(j) + \" are Matched\")\n",
    "                    face_matched = True\n",
    "                \n",
    "        if not face_matched:\n",
    "            print(\"no face matched\")\n",
    "                                \n",
    "    #highlight_faces(frame, faces)\n",
    "    \n",
    "    for face in face_infos:\n",
    "        x, y, width, height = face['box']\n",
    "        cv2.rectangle(frame, (x,y), (x + width, y + height), color=(0,0,255), thickness=4)\n",
    "        \n",
    "    cv2.imshow(\"frame\", frame)\n",
    "    # press ESC to close window\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1758b28b-1bed-4773-8661-84b5bea3e8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
